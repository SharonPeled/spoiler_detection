{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125480/125480 [01:46<00:00, 1173.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17924/17924 [00:14<00:00, 1204.56it/s]\n",
      "100%|██████████| 895248/895248 [00:08<00:00, 101616.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpoilerNet(\n",
       "  (word_embedding): Embedding(895248, 300)\n",
       "  (word_gru): GRU(303, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (attention): Linear(in_features=50, out_features=1, bias=False)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (sentence_gru): GRU(100, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (output_layer): Linear(in_features=100, out_features=2, bias=True)\n",
       "  (logsoftmax_out): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SpoilerDataset(filename=\"train.pickle\", load=LOAD_PROCESSED_DATA) # using defaults\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "\n",
    "valid_small_dataset = SpoilerDataset(filename=\"valid.pickle\", load=LOAD_PROCESSED_DATA)\n",
    "valid_small_dataloader = DataLoader(valid_small_dataset)\n",
    "\n",
    "model = SpoilerNet(train_dataset, WORD_EMBEDDING_DIM, WORD_FEATURES_DIM, HIDDEN_DIM,len(train_dataset.word_to_id), 2, True)\n",
    "\n",
    "model.zero_grad()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 895248/895248 [00:08<00:00, 110378.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpoilerNet(\n",
       "  (word_embedding): Embedding(895248, 300)\n",
       "  (word_gru): GRU(303, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (attention): Linear(in_features=50, out_features=1, bias=False)\n",
       "  (tanh): Tanh()\n",
       "  (softmax): Softmax(dim=1)\n",
       "  (sentence_gru): GRU(100, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (output_layer): Linear(in_features=100, out_features=2, bias=True)\n",
       "  (logsoftmax_out): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SpoilerNet(train_dataset, WORD_EMBEDDING_DIM, WORD_FEATURES_DIM, HIDDEN_DIM,len(train_dataset.word_to_id), 2, True)\n",
    "\n",
    "model.zero_grad()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1,square=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.square = square\n",
    "\n",
    "    def forward(self, predictions, targets):   \n",
    "        inputs = predictions[:,1]\n",
    "        input_flat = inputs.view(-1)\n",
    "        target_flat = targets.view(-1)\n",
    "        mult = (input_flat * target_flat).sum()\n",
    "        if self.square:\n",
    "            input_flat = torch.mul(input_flat,input_flat)\n",
    "            target_flat = torch.mul(target_flat,target_flat)\n",
    "        dice = (2.*mult + self.smooth)/(input_flat.sum() + target_flat.sum() + self.smooth)\n",
    "        print(inputs)\n",
    "        print(input_flat)\n",
    "        print(targets)\n",
    "        print(target_flat)\n",
    "        print(mult)\n",
    "        print(dice)\n",
    "        return 1-dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self,gamma):\n",
    "        super(FocalLoss,self).__init__()\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self,predictions,targets):\n",
    "        inputs = predictions[:,1]\n",
    "        input_flat = inputs.view(-1)\n",
    "        target_flat = targets.view(-1).float()\n",
    "        bce = nn.BCELoss(reduction='mean')(input_flat,target_flat)\n",
    "        \n",
    "        bce_exp = torch.exp(-bce)\n",
    "        fc = ((1-bce_exp)**self.gamma) * bce\n",
    "        return fc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSC(nn.Module):\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DSC, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        inputs = predictions[:,1]\n",
    "        input_flat = inputs.view(-1)\n",
    "        target_flat = targets.view(-1)\n",
    "        mult = ((1-input_flat) * input_flat * target_flat)\n",
    "        dice_upper = (2.*(mult.sum()) + self.smooth)\n",
    "        dice_bottom = ((1-input_flat)*(input_flat)).sum() + target_flat.sum() + self.smooth\n",
    "        dice = dice_upper/dice_bottom\n",
    "        print(inputs)\n",
    "        print(input_flat)\n",
    "        print(target_flat)\n",
    "        print(mult)\n",
    "        print(1-input_flat)\n",
    "        return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4936, 0.4937, 0.5015, 0.4967], device='cuda:0',\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.4936, 0.4937, 0.5015, 0.4967], device='cuda:0',\n",
      "       grad_fn=<ViewBackward>)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0., 0., 0., 0.], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([0.5064, 0.5063, 0.4985, 0.5033], device='cuda:0',\n",
      "       grad_fn=<RsubBackward1>)\n",
      "tensor(0.5000, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = DSC()\n",
    "for input_data in valid_small_dataloader:\n",
    "    review = input_data[0]\n",
    "    labels = torch.tensor(input_data[1]).to(model.device)\n",
    "    book_id = input_data[2]\n",
    "    user_id = input_data[3]\n",
    "\n",
    "    scores, output = model(review, book_id, user_id)\n",
    "    print(loss(scores,labels))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 1., 0., 0., 0.], device='cuda:0')\n",
      "tensor([0., 0., 1., 0., 0., 0.], device='cuda:0')\n",
      "tensor([1, 1, 1, 0, 0, 0], device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "tensor([1., 1., 0., 1., 1., 1.], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.2500, device='cuda:0')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(torch.tensor([[1.0, 0],[1.0, 0],[0, 1.0],[1.0, 0],[1.0,0],[1.0,0]]).to(model.device),torch.tensor([1,1,1,0,0,0]).to(model.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "from dataset import SpoilerDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUFFIX_ALL = \"\"\n",
    "EPOCHS = (0,10)\n",
    "WORD_EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 50\n",
    "WORD_FEATURES_DIM = 3\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # torch.device(\"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "LEARNING_RATE = 0.001\n",
    "ACCUMULATE_GRAD_STEPS = 128\n",
    "LOAD_PROCESSED_DATA = False\n",
    "SAVE_MODEL = True\n",
    "SAVE_LOG_TO_FILE = True\n",
    "NEGATIVE_CLASS_WEIGHT = 0.5\n",
    "LOAD_MODEL = False\n",
    "MODEL_VERSION = 10\n",
    "TRAIN_TIME_THRESHOLD = 5000\n",
    "OUT_FILE = \"output_V5.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpoilerNet(nn.Module):\n",
    "    def __init__(self, train_dataset, word_emb_dim, features_dim, hidden_dim, word_vocab_size, num_gru_layers, bidirectional):\n",
    "        super(SpoilerNet, self).__init__()\n",
    "        self.device = DEVICE\n",
    "        self.dataset = train_dataset\n",
    "        pretrained_embeddings = self.dataset.load_pretrained_embeddings(word_emb_dim)\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(pretrained_embeddings.to(self.device), freeze=True)\n",
    "        # adding word features\n",
    "        word_emb_dim += features_dim\n",
    "        self.word_gru = nn.GRU(input_size=word_emb_dim, hidden_size=hidden_dim, \n",
    "                               num_layers=num_gru_layers, bidirectional=bidirectional ,batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=2*hidden_dim,out_features=hidden_dim)\n",
    "        self.attention = nn.Linear(in_features=hidden_dim,out_features=1,bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sentence_gru = nn.GRU(input_size=2*hidden_dim,hidden_size=hidden_dim,num_layers=num_gru_layers,\n",
    "                                   bidirectional=bidirectional,batch_first=True)\n",
    "        self.output_layer = nn.Linear(in_features=2*hidden_dim,out_features=2)\n",
    "        self.book_bias = nn.Parameter(torch.torch.zeros(self.dataset.get_num_books()), requires_grad=True)\n",
    "        self.user_bias = nn.Parameter(torch.torch.zeros(self.dataset.get_num_users()), requires_grad=True)\n",
    "        self.logsoftmax_out = nn.Softmax()\n",
    "        \n",
    "    def forward(self,review, book_ind, user_ind):\n",
    "        vectorized_sentences = []\n",
    "        for sentence in review:\n",
    "            sentence = sentence.to(self.device)\n",
    "            embedded_sentence = self.word_embedding(sentence).to(self.device)\n",
    "            word_features = self.dataset.get_tf_idf_features_tensor(sentence, book_ind).to(self.device)\n",
    "            embedded_sentence = torch.cat([embedded_sentence.float(), word_features.float()],dim=2)\n",
    "            word_hidden_state, _ = self.word_gru(embedded_sentence)\n",
    "            mu = self.tanh(self.linear(word_hidden_state))\n",
    "            alpha_weights = self.softmax(self.attention(mu))\n",
    "            attended_vector = (alpha_weights * word_hidden_state).sum(dim=1)\n",
    "            vectorized_sentences.append(attended_vector)\n",
    "        stacked_vectorized_sentences = torch.stack(vectorized_sentences,dim=1)\n",
    "        sentence_hidden_state , _ = self.sentence_gru(stacked_vectorized_sentences)\n",
    "        output = self.output_layer(sentence_hidden_state).view(len(review),-1)\n",
    "        output += self.book_bias[book_ind] + self.book_bias[user_ind]\n",
    "        scores = self.logsoftmax_out(output)\n",
    "        return scores,output\n",
    "\n",
    "    def predict(self,dataloader):\n",
    "        self.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_probs = []\n",
    "        softmax = nn.Softmax()\n",
    "        for batch_idx, input_data in enumerate(dataloader):\n",
    "#         for batch_idx, input_data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            review = input_data[0]\n",
    "            labels = torch.tensor(input_data[1]).to(self.device)\n",
    "            book_id = input_data[2]\n",
    "            user_id = input_data[3]\n",
    "            scores, ouput = self.forward(review, book_id, user_id)\n",
    "            probs = softmax(scores)\n",
    "            positive_probs = probs[:,1]\n",
    "            _, predicted = torch.max(probs.data, 1)\n",
    "            y_true += labels.tolist()\n",
    "            y_pred += predicted.tolist()\n",
    "            y_probs += positive_probs.tolist()\n",
    "        metrics = calc_metrics(y_true, y_pred, y_probs)\n",
    "        metrics_str = json.dumps(metrics, indent=4)\n",
    "        if SAVE_LOG_TO_FILE:\n",
    "            write_to_log(metrics_str)\n",
    "        print(metrics_str)\n",
    "        self.train()\n",
    "        return y_true, y_pred, metrics\n",
    "\n",
    "\n",
    "def calc_metrics(y_true, y_pred, y_probs):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
    "    auc = roc_auc_score(y_true,y_probs)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    mat = np.array([tn, fp, fn, tp])\n",
    "    mat = mat/mat.sum()\n",
    "    matrics = np.concatenate([np.array([auc, precision, recall, f1, accuracy]), mat]).round(2)\n",
    "    metrics = {\"AUC\":matrics[0], \"precision\": matrics[1], \"recall\":matrics[2], \"f1\":matrics[3], \"accuracy\":matrics[4],\n",
    "               \"TN\":matrics[5], \"FP\":matrics[6], \"FN\":matrics[7], \"TP\":matrics[8],\n",
    "               \"Neg-Pos (pred)\": f\"[{round((len(y_pred)-sum(y_pred))/len(y_pred),2)}, {round(sum(y_pred)/len(y_pred),2)}]\",\n",
    "               \"Neg-Pos (true)\": f\"[{round((len(y_true)-sum(y_true))/len(y_true),2)}, {round(sum(y_true)/len(y_true),2)}]\",\n",
    "               \"Sum\": len(y_true)}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_log(obj):\n",
    "    if not SAVE_LOG_TO_FILE:\n",
    "        return\n",
    "    with open(OUT_FILE, \"a\") as file:\n",
    "        file.write(str(obj))\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def train():\n",
    "    write_to_log(\"--\"*50)\n",
    "    write_to_log(\"--\"*50)\n",
    "    write_to_log(\"New Training \\t\"*5)\n",
    "    train_dataset = SpoilerDataset(filename=\"train.pickle\", load=LOAD_PROCESSED_DATA) # using defaults\n",
    "    train_dataloader = DataLoader(train_dataset)\n",
    "    \n",
    "    valid_small_dataset = SpoilerDataset(filename=\"valid.pickle\", load=LOAD_PROCESSED_DATA)\n",
    "    valid_small_dataloader = DataLoader(valid_small_dataset)\n",
    "    \n",
    "    model = SpoilerNet(train_dataset, WORD_EMBEDDING_DIM, WORD_FEATURES_DIM, HIDDEN_DIM,len(train_dataset.word_to_id), 2, True)\n",
    "    \n",
    "    if LOAD_MODEL:\n",
    "        load_model(model,MODEL_VERSION)\n",
    "        model.train()\n",
    "        \n",
    "    model.zero_grad()\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # reduction='sum' because we're using 1 sample batch\n",
    "    criterion = nn.NLLLoss(weight=torch.tensor([NEGATIVE_CLASS_WEIGHT, 1]), reduction='sum').to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    beg = time.time()\n",
    "    loss_train_list = []\n",
    "    for epoch in range(*EPOCHS):\n",
    "        loss_train_total = 0\n",
    "        # each batch is a single review (many sentences)\n",
    "        for batch_idx, input_data in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            \n",
    "            review = input_data[0]\n",
    "            labels = torch.tensor(input_data[1]).to(model.device)\n",
    "            book_id = input_data[2]\n",
    "            user_id = input_data[3]\n",
    "            \n",
    "            scores, output = model(review, book_id, user_id)\n",
    "            loss = criterion(scores, labels)\n",
    "            loss = loss/ACCUMULATE_GRAD_STEPS\n",
    "            loss.backward()\n",
    "            \n",
    "            if batch_idx % ACCUMULATE_GRAD_STEPS == 0:\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "\n",
    "            loss_train_total += loss.item()\n",
    "#             break\n",
    "            if time.time()-beg > TRAIN_TIME_THRESHOLD:\n",
    "                _, _, _ = model.predict(valid_small_dataloader)\n",
    "                write_to_log(str(round(time.time()-beg,2)) + \"\\t\" + str(batch_idx))\n",
    "                write_to_log(\"\\n\")\n",
    "                beg = time.time()\n",
    "\n",
    "        if SAVE_MODEL:\n",
    "            save_model(model, epoch+1)\n",
    "\n",
    "        loss_train_total = loss_train_total / len(train_dataset)\n",
    "        loss_train_list.append(float(loss_train_total))\n",
    "\n",
    "        print(\"Epoch {} Completed,\\tTrain Loss: {}\".format(epoch + 1, np.mean(loss_train_list[-batch_idx:])))\n",
    "        \n",
    "        if SAVE_LOG_TO_FILE:\n",
    "            write_to_log(\"Epoch {} Completed,\\tTrain Loss: {}\".format(epoch + 1, np.mean(loss_train_list[-batch_idx:])))\n",
    "            \n",
    "    return loss_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 1638.07it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1890.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ...\n",
      "Generating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12826/12826 [00:00<00:00, 190048.73it/s]\n",
      "100%|██████████| 90/90 [00:05<00:00, 17.21it/s]\n",
      "  3%|▎         | 3/90 [00:00<00:03, 25.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Completed,\tTrain Loss: 0.0425466095821725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:05<00:00, 17.82it/s]\n",
      "  3%|▎         | 3/90 [00:00<00:03, 28.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Completed,\tTrain Loss: 0.04128060535941687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:05<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Completed,\tTrain Loss: 0.040712067147682385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0425466095821725, 0.040014601136661235, 0.03957499072421342]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 1668.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 12826/12826 [00:00<00:00, 192238.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpoilerDataset(filename=\"train.pickle\", load=LOAD_PROCESSED_DATA) # using defaults\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "model = SpoilerNet(train_dataset, WORD_EMBEDDING_DIM, WORD_FEATURES_DIM, HIDDEN_DIM,len(train_dataset.word_to_id), 2, True)\n",
    "load_model(model,MODEL_VERSION)\n",
    "model.to(DEVICE)\n",
    "write_to_log(\"\\n\\nStart Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 1706.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = SpoilerDataset(filename=\"valid.pickle\", load=LOAD_PROCESSED_DATA)\n",
    "valid_dataloader = DataLoader(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AUC\": 0.56,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"accuracy\": 0.91,\n",
      "    \"TN\": 0.91,\n",
      "    \"FP\": 0.0,\n",
      "    \"FN\": 0.09,\n",
      "    \"TP\": 0.0,\n",
      "    \"Neg-Pos (pred)\": \"[1.0, 0.0]\",\n",
      "    \"Neg-Pos (true)\": \"[0.91, 0.09]\",\n",
      "    \"Sum\": 159\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = model.predict(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 1177.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SpoilerDataset(filename=\"test.pickle\", load=LOAD_PROCESSED_DATA)\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AUC\": 0.49,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"accuracy\": 0.93,\n",
      "    \"TN\": 0.93,\n",
      "    \"FP\": 0.0,\n",
      "    \"FN\": 0.07,\n",
      "    \"TP\": 0.0,\n",
      "    \"Neg-Pos (pred)\": \"[1.0, 0.0]\",\n",
      "    \"Neg-Pos (true)\": \"[0.93, 0.07]\",\n",
      "    \"Sum\": 553\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = model.predict(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:00<00:00, 1676.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpoilerDataset(filename=\"train.pickle\", load=LOAD_PROCESSED_DATA)\n",
    "train_dataloader = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"AUC\": 0.45,\n",
      "    \"precision\": 0.0,\n",
      "    \"recall\": 0.0,\n",
      "    \"f1\": 0.0,\n",
      "    \"accuracy\": 0.86,\n",
      "    \"TN\": 0.86,\n",
      "    \"FP\": 0.0,\n",
      "    \"FN\": 0.14,\n",
      "    \"TP\": 0.0,\n",
      "    \"Neg-Pos (pred)\": \"[1.0, 0.0]\",\n",
      "    \"Neg-Pos (true)\": \"[0.86, 0.14]\",\n",
      "    \"Sum\": 1359\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = model.predict(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
