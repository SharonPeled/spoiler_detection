{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "from SpoilerDataset import SpoilerDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143403\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SpoilerDataset('data/train_reviews_balanced.json','data/word_to_id.pickle','data/id_to_word.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpoilerNet(nn.Module):\n",
    "    def __init__(self,word_emb_dim, hidden_dim, word_vocab_size):\n",
    "        super(SpoilerNet, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_emb_dim)\n",
    "        self.word_gru = nn.GRU(input_size=word_emb_dim,hidden_size=hidden_dim,num_layers=2,bidirectional=True,batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=2*hidden_dim,out_features=hidden_dim)\n",
    "        self.attention = nn.Linear(in_features=hidden_dim,out_features=1,bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.sentence_gru = nn.GRU(input_size=2*hidden_dim,hidden_size=hidden_dim,num_layers=2,bidirectional=True,batch_first=True)\n",
    "        self.output_layer = nn.Linear(in_features=2*hidden_dim,out_features=2)\n",
    "        self.softmax_output = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self,review):\n",
    "        vectorized_sentences = []\n",
    "        for sentence in review:\n",
    "            sentence = sentence.to(self.device)\n",
    "            embedded_sentence = self.word_embedding(sentence)\n",
    "            word_hidden_state, _ = self.word_gru(embedded_sentence)\n",
    "            mu = self.tanh(self.linear(word_hidden_state))\n",
    "            alpha_weights = self.softmax(self.attention(mu))\n",
    "            attended_vector = (alpha_weights * word_hidden_state).sum(dim=1)\n",
    "            vectorized_sentences.append(attended_vector)\n",
    "        \n",
    "        stacked_vectorized_sentences = torch.stack(vectorized_sentences,dim=1)\n",
    "        sentence_hidden_state , _ = self.sentence_gru(stacked_vectorized_sentences)\n",
    "        output = self.output_layer(sentence_hidden_state).view(len(review),-1)\n",
    "        probs = self.softmax_output(output)\n",
    "        return probs,output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "WORD_EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 50\n",
    "\n",
    "word_vocab_size = len(train_dataset.word_to_id)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = SpoilerNet(WORD_EMBEDDING_DIM,HIDDEN_DIM,word_vocab_size)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "acumulate_grad_steps = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.conda/envs/IIS/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "loss_list_train = []\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_train_total = 0\n",
    "    i = 0\n",
    "    for batch_idx, input_data in enumerate(train_dataloader):\n",
    "        i += 1\n",
    "        review = input_data[0]\n",
    "        labels = torch.tensor(input_data[1]).to(model.device)\n",
    "        probs,output = model(review)\n",
    "        loss = criterion(probs,labels)\n",
    "        loss = loss/ acumulate_grad_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % acumulate_grad_steps == 0:\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "        \n",
    "        loss_train_total += loss.item()\n",
    "            \n",
    "    \n",
    "    loss_train_total = loss_train_total / len(train_dataset)\n",
    "    loss_list_train.append(float(loss_train_total))\n",
    "    e_interval = i\n",
    "    print(\"Epoch {} Completed,\\tTrain Loss: {}\".format(epoch + 1, np.mean(loss_list_train[-e_interval:])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.conda/envs/IIS/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.921875\n"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "positive = 0\n",
    "for batch_idx, input_data in enumerate(train_dataloader):\n",
    "\n",
    "    review = input_data[0]\n",
    "    labels = torch.tensor(input_data[1]).to(model.device)\n",
    "    probs,ouput = model(review)\n",
    "    _, predicted = torch.max(probs.data, 1)\n",
    "    for index,label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            if predicted[index]==1:\n",
    "                true_positive +=1\n",
    "            positive += 1\n",
    "            \n",
    "\n",
    "print(\"Precision: {}\".format(true_positive/positive))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
