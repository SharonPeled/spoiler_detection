{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from functools import partial\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_small_subset(dataset_path, size=1000000):\n",
    "    with open(DATASET_PATH) as json_file:\n",
    "        with open(dataset_path, 'w') as new_file:\n",
    "            for _, line in zip(range(size), json_file):\n",
    "                new_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_vocabulary(word_count, size):\n",
    "    size = min(size,len(word_count))\n",
    "    id_to_word = {} # to pharse the sentences\n",
    "    word_to_id = defaultdict(partial(map_to_not_in_vocabulary, size)) # for loading embeddings\n",
    "    for ind, (word, _) in enumerate(word_count.most_common(size)):\n",
    "        id_to_word[ind] = word\n",
    "        word_to_id[word] = ind\n",
    "    id_to_word[size] = SPECIAL_CHARACTER\n",
    "    return id_to_word, word_to_id\n",
    "\n",
    "\n",
    "def save_dictionaries(dataset_path):\n",
    "    word_count = Counter()\n",
    "    book_to_id = {}\n",
    "    user_to_id = {}\n",
    "    book_reviews_count = defaultdict(int)\n",
    "    word_per_book_count = defaultdict(Counter)\n",
    "    with open(dataset_path) as json_file:\n",
    "        for line in json_file:\n",
    "            data_dict = json.loads(line)\n",
    "            book_id_org = data_dict['book_id']\n",
    "            user_id = data_dict['user_id']\n",
    "            book_to_id[book_id_org] = book_to_id.get(book_id_org, len(book_to_id))\n",
    "            user_to_id[user_id] = user_to_id.get(user_id, len(user_to_id))\n",
    "            book_id = book_to_id[book_id_org]\n",
    "            review_sentences = data_dict['review_sentences']\n",
    "            book_reviews_count[book_id] += 1\n",
    "            for _, sentence in review_sentences:\n",
    "                for word in sentence.split():\n",
    "                    word = clean_word(word)\n",
    "                    word_count[word] += 1\n",
    "                    word_per_book_count[word][book_id] += 1\n",
    "                    \n",
    "    id_to_word, word_to_id = cut_vocabulary(word_count, size=VOCAB_SIZE)\n",
    "    print(f\"Vocab size: {len(word_count)}\")\n",
    "    save_pickle(word_to_id, \"word_to_id.pickle\")\n",
    "    save_pickle(id_to_word, \"id_to_word.pickle\")\n",
    "    save_pickle(word_count, \"word_count.pickle\")\n",
    "    save_pickle(book_to_id, \"book_to_id.pickle\")\n",
    "    save_pickle(user_to_id, \"user_to_id.pickle\")\n",
    "    save_pickle(book_reviews_count, \"book_reviews_count.pickle\")\n",
    "    save_pickle(word_per_book_count, \"word_per_book_count.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_samples(dataset_path):\n",
    "    true_samples = []\n",
    "    false_samples = []\n",
    "    with open(dataset_path) as json_file:\n",
    "        for line in json_file:\n",
    "            sample = json.loads(line)\n",
    "            if not sample['has_spoiler']:\n",
    "                false_samples.append(sample)\n",
    "            else:\n",
    "                true_samples.append(sample)\n",
    "    random.shuffle(true_samples)\n",
    "    random.shuffle(false_samples)\n",
    "    return true_samples, false_samples\n",
    "\n",
    "\n",
    "def split_samples(samples, rat1, rat2):\n",
    "    first_chunk = int(rat1*len(samples))\n",
    "    second_chunk = int(rat2*len(samples))\n",
    "    return samples[first_chunk+second_chunk:], samples[:first_chunk] , samples[first_chunk:first_chunk+second_chunk]\n",
    "\n",
    "\n",
    "def train_valid_test_split(dataset_path, test_ratio, valid_ratio, class_ratio=1.0): \n",
    "    # class_ratio = 1 - means same number of sample for each class\n",
    "    true_samples, false_samples = fetch_samples(dataset_path)\n",
    "    if class_ratio is None:\n",
    "        train, valid, test = split_samples(false_samples+true_samples,test_ratio,valid_ratio)\n",
    "    else:\n",
    "        size = int(min(len(true_samples), len(false_samples))/class_ratio)\n",
    "        print(f\"size: size\")\n",
    "        true_train, true_valid, true_test = split_samples(true_samples[:size],test_ratio,valid_ratio)\n",
    "        false_train, false_valid, false_test = split_samples(false_samples[:size],test_ratio,valid_ratio)\n",
    "        train, valid, test = true_train+false_train, true_valid+false_valid, true_test+false_test\n",
    "    random.shuffle(train)\n",
    "    random.shuffle(valid)\n",
    "    random.shuffle(test)\n",
    "    print(f\"Train size: {len(train)}\")\n",
    "    print(f\"Valid size: {len(valid)}\")\n",
    "    print(f\"Test size: {len(test)}\")\n",
    "    save_pickle(train, \"train.pickle\")\n",
    "    save_pickle(valid, \"valid.pickle\")\n",
    "    save_pickle(test, \"test.pickle\")\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_small_subset(SAMPLE_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 712166\n"
     ]
    }
   ],
   "source": [
    "save_dictionaries(SAMPLE_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: size\n",
      "Train size: 77622\n",
      "Valid size: 25874\n",
      "Test size: 25874\n"
     ]
    }
   ],
   "source": [
    "_, _, _ = train_valid_test_split(SAMPLE_DATASET_PATH, TEST_RATIO, VALID_RATIO, class_ratio=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
